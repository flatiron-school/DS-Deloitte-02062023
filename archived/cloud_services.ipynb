{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import boto3\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nerds.net/wp-content/uploads/2018/02/cloud-computer-reality-750x646.jpg\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SWBAT:\n",
    " - Use the `pickle` library to read and write files;\n",
    " - Explain the general concept of \"the cloud\";\n",
    " - Understand the cases where _hardware acceleration_ is useful;\n",
    " - Understand the cases where _cloud storage_ is useful;\n",
    " - Explain the purpose of deploying a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "There are several services available in the cloud:\n",
    "\n",
    "1. Hardware acceleration\n",
    "2. Cloud storage\n",
    "3. Model serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Acceleration\n",
    "\n",
    "As much as software libraries like NumPy or Spark can improve the efficiency of code, there is a limit to how much of a difference they can make, depending on the actual hardware of your computer.\n",
    "\n",
    "As a general concept, [hardware acceleration](https://www.omnisci.com/learn/resources/technical-glossary/hardware-acceleration) means using purpose-built hardware rather than general-purpose hardware.\n",
    "\n",
    "In the case of machine learning, this typically means running your code on a GPU, rather than a CPU.  A CPU _can_ do everything that a GPU can do, but it is not optimized for it, so it will likely take more time.  [This blog](https://towardsdatascience.com/maximize-your-gpu-dollars-a9133f4e546a) argues that a CPU is to a GPU as a horse and buggy is to a car.\n",
    "\n",
    "Sometimes you want to use cloud computing just for the GPU!\n",
    "\n",
    "### Cloud Services with Hardware Acceleration\n",
    "\n",
    "There is a lot of overlap between these services and the services listed on the Deployment page, but you may need to configure them specifically to indicate that you want to use a GPU:\n",
    "\n",
    " - [AWS EC2](https://aws.amazon.com/blogs/machine-learning/train-deep-learning-models-on-gpus-using-amazon-ec2-spot-instances/)\n",
    " - [Google Cloud Platform](https://cloud.google.com/ml-engine/docs/using-gpus)\n",
    " - [IBM Watson Studio](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ml_dlaas_gpus.html)\n",
    " - [Azure VM](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-gpu)\n",
    "\n",
    "There are also some cloud IDEs with GPUs, where you can run Jupyter Notebook code:\n",
    "\n",
    " - [AWS Sagemaker](https://aws.amazon.com/machine-learning/accelerate-machine-learning-P3/) (there are labs in Canvas introducing Sagemaker — it is fairly fast and easy to use, but can get expensive quickly with large datasets)\n",
    " - [Databricks Community Edition](https://community.cloud.databricks.com/)\n",
    " - [Google Colab](https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c)\n",
    " - [Kaggle kernels](https://www.kaggle.com/dansbecker/running-kaggle-kernels-with-a-gpu)\n",
    "\n",
    "You can also upload Jupyter notebooks to data.world.\n",
    "\n",
    "Because there is a GPU available in the free tier, Google Colab is the most popular of these tools for our students. [Here](https://medium.com/@sam.bbmgmt/integrating-google-drive-colab-with-github-bffaca97eb5b) is a blog post from a former student explaining how to use the file system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo\n",
    "\n",
    "Let's upload the same notebook to a couple different platforms, as an illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Storage\n",
    "\n",
    "It's annoying to have huge data files taking up space on your laptop, and if you want to train your model in the cloud, your data also needs to be in the cloud.  But for reasons related to hardware acceleration, it can get pretty expensive to store large datasets in general-purpose cloud services like an EC2 instance or a cloud VM.  That's when cloud storage services become useful.\n",
    "\n",
    "### Cloud Storage Buckets\n",
    "\n",
    "The major providers of storage \"buckets\" are:\n",
    "\n",
    " - [AWS S3](https://aws.amazon.com/s3/getting-started/)\n",
    " - [Google Cloud Storage](https://cloud.google.com/storage/)\n",
    " - [Azure Storage](https://docs.microsoft.com/en-us/azure/storage/common/storage-introduction)\n",
    "\n",
    "These tools are designed for uploads of raw files, e.g. folders full of images, CSVs, or JSONs.\n",
    "\n",
    "They each cost about 2-5 cents per GB per month.  AWS S3 is the oldest and tends to have the most integration support with other platforms, although you may need to use Google storage if you're using other Google products or Azure storage if you're using other Azure products.\n",
    "\n",
    "### Cloud Databases\n",
    "\n",
    "If you want to deploy a website where new information gets saved (what kinds of queries users perform, user ratings of the quality of predictions, etc.) then you need a cloud database.  These work roughly the same as a database running on your computer.\n",
    "\n",
    "Using a cloud database is mainly an opportunity to practice using tooling that you are likely to use on the job, because they assist with collaboration.\n",
    "\n",
    "Some popular providers are:\n",
    "\n",
    " - [Heroku Postgres](https://www.heroku.com/postgres)\n",
    " - [MongoDB Atlas](https://www.mongodb.com/cloud/atlas)\n",
    " - [AWS Aurora](https://aws.amazon.com/rds/aurora/)\n",
    " - [AWS RDS](https://aws.amazon.com/rds/)\n",
    "\n",
    "Most of these tools have a free tier, which permits a limited number of records to be stored.\n",
    "\n",
    "### Links to Other Resources\n",
    "\n",
    " - [Introduction to AWS for Data Scientists](https://www.dataquest.io/blog/introduction-to-aws-for-data-scientists/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "Here we have resources for three different techniques for deploying a machine learning model:\n",
    "1. Full-stack web application\n",
    "2. Cloud function\n",
    "3. ML-specific deployment\n",
    "\n",
    "### Full-Stack Web Application Example: Flask\n",
    "Flask means developing a full-stack web application. It follows a model-view-controller (MVC) pattern and requires that you sometimes have to follow a \"convention over configuration\" pattern and put certain files in certain folders.\n",
    "\n",
    "There are two main ways I recommend deploying a Flask app: [Heroku](https://devcenter.heroku.com/articles/deploying-python), or [AWS EC2](https://www.codementor.io/dushyantbgs/deploying-a-flask-application-to-aws-gnva38cf0). The main difference is that Heroku uses \"dynos\", a type of container that gets fully re-created when the site is \"woken up\" based on certain config files it finds in a GitHub repo.  EC2 is more like a \"real\" computer, where you can SSH in and download things, log out, log back in, and those things will still be there.  EC2 gives you more configuration capabilities and more computational power, but also requires more setup than Heroku.\n",
    "\n",
    "#### Pricing\n",
    "[Heroku free tier](https://www.heroku.com/pricing) allows one web app for free. The [AWS free tier](https://aws.amazon.com/ec2/pricing/) allows 750 hours of \"micro\" level server time per month.\n",
    "\n",
    "#### Pros\n",
    " - Free\n",
    " - Allows you to become more familiar with web development and have a shared vocabulary with software developers\n",
    " - You can generate data visualizations in Python (don't need to learn JavaScript, only HTML and CSS)\n",
    " - Supports a multi-step process with more than one user interaction\n",
    " - Single solution to get a live user interface you can link from your portfolio\n",
    "\n",
    "#### Cons\n",
    " - Learning curve for all of Flask can be steep\n",
    " - More systems administration work than a cloud function or ML model\n",
    " - Less realistic workflow (unless you are the only technical person on your team, you will probably not be expected to set up a web server)\n",
    " \n",
    "#### Documentation\n",
    " - [Flask app repo from 0624 cohort](https://github.com/learn-co-students/capstone-flask-app-template-seattle-ds-062419)\n",
    " - [Flask quickstart documentation](https://flask.palletsprojects.com/en/1.1.x/quickstart/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flask Demo\n",
    "\n",
    "Political NLP Predictor at `~/flatiron/inauguralNLP`:\n",
    "\n",
    "Let's\n",
    "- check out the \"app.py\" file; and\n",
    "- run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling for Deployment Example\n",
    "\n",
    "This shows the basic outline for training a model, evaluating it, then using it in a \"production\" context to make predictions about new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extract, Transform, Load Data\n",
    "\n",
    "This is easy here because I'm using a nice tidy dataset from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get premade wine dataset from sklearn\n",
    "\n",
    "data = load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build a Model to Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build a model to predict the class of wine\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "classifier = RandomForestClassifier(max_depth=2, random_state=0, n_estimators=100)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate the Model\n",
    "\n",
    "Not necessarily the most realistic performance, but let's go with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test, classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Export the Model\n",
    "\n",
    "As far as I can tell, the [`pickle` format](https://docs.python.org/3/library/pickle.html) is most popular for this task in Python right now. Pickling is a form of serialization or flattening, which basically means converting everything about an object in memory into bits of data that can be stored in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"wine_classifier.pickle\", \"wb\") # \"wb\" means \"write as bytes\"\n",
    "pickle.dump(classifier, output_file)\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Load the Model\n",
    "\n",
    "This part would actually almost never be in the same file as the previous step. The goal is to take information that was stored in memory at one time, then save it so it can be used later. Here specifically this is useful because training a model is usually a lot slower than using the model to make a prediction, so this saves us from having to re-run that costly operation each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = open(\"wine_classifier.pickle\", \"rb\") # \"rb\" means \"read as bytes\"\n",
    "loaded_model = pickle.load(model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Make a Prediction with the Loaded Model\n",
    "\n",
    "In this section I'm constructing a request JSON that resembles what would come from a user who wants a predicted class of wine based on these feature values. This code would not actually exist in your deployed application, it would be created automatically by whatever protocol generated the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a fake request JSON from the user with all the headings\n",
    "\n",
    "request_json = {}\n",
    "\n",
    "expected_features = (\"Alcohol\", \"Malic acid\", \"Ash\", \"Alcalinity of ash\", \\\n",
    "        \"Magnesium\", \"Total phenols\", \"Flavanoids\", \"Nonflavanoid phenols\", \\\n",
    "        \"Proanthocyanins\", \"Color intensity\", \"Hue\", \\\n",
    "        \"OD280/OD315 of diluted wines\", \"Proline\")\n",
    "example_values = [1.282e+01, 3.370e+00, 2.300e+00, 1.950e+01, 8.800e+01, 1.480e+00, \\\n",
    "       6.600e-01, 4.000e-01, 9.700e-01, 1.026e+01, 7.200e-01, 1.750e+00, \\\n",
    "       6.850e+02]\n",
    "\n",
    "for i, feature in enumerate(expected_features):\n",
    "    request_json[feature] = example_values[i]\n",
    "request_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the section that more closely resembles what you might have in your application. I'm checking to make sure that the expected values are in the request_json, transforming them into the right format to make a prediction, then printing out that prediction. In your actual deployed code, you would most likely be **returning** the response, not printing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if request_json and all(feature in request_json for feature in expected_features):\n",
    "    \n",
    "    # unpack all of the relevant values from the request into a list\n",
    "    \n",
    "    test_value = [request_json[feature] for feature in expected_features]\n",
    "    \n",
    "    # make a prediction from the \"user input\"\n",
    "    \n",
    "    predicted_class = int(loaded_model.predict([test_value])[0])\n",
    "    \n",
    "    # construct a response\n",
    "    \n",
    "    response_json = {\"prediction\": predicted_class}\n",
    "    print(response_json)\n",
    "else:\n",
    "    print(\"something was missing from the request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS S3 Buckets Example\n",
    "\n",
    "### Assumptions\n",
    " - Working from a Jupyter notebook locally\n",
    " - Not concerned about access or keeping data private\n",
    "\n",
    "For the purpose of this lesson, the `wine_classifier.pickle` file has already been uploaded to Nick's AWS account.  To make your pickle file available from code, you would need to make an account and upload the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = \"pickle-bucket\"\n",
    "PICKLE_FILE = \"wine_classifier.pickle\"\n",
    "CSV_FILE = \"wine.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boto 3\n",
    "This is an SDK for connecting Python to S3 buckets.  Docs [here](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html). This is a good tool for large pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install boto3 -y\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "obj = s3.Object(BUCKET, PICKLE_FILE)\n",
    "\n",
    "obj.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_body = obj.get()[\"Body\"].read()\n",
    "response_body[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.loads(response_body)\n",
    "\n",
    "loaded_model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "If you are trying to load a CSV rather than a pickle, you can do that directly in Pandas!  First you'll need to install `S3FS` (docs [here](https://s3fs.readthedocs.io/en/latest/)) which allows Python to access S3 buckets like they are part of the local file system.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install s3fs -c conda-forge -y\n",
    "\n",
    "import s3fs\n",
    "\n",
    "df = pd.read_csv(f's3://{BUCKET}/{CSV_FILE}', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level-Ups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Function Example: Google Cloud Functions\n",
    "\n",
    "Google Cloud Functions allow you to make a single function that can be called from a REST API interface.  The Python language versions are actually running on top of Flask, with as much boilerplate removed as possible.  So you still need to learn some Flask to be able to get everything working.\n",
    "\n",
    "#### Pricing\n",
    "\n",
    "Up to 2 million free Cloud Function invocations per month. The [free tier](https://cloud.google.com/free/) requires signing up with a credit card, but promises \"No autocharge after free trial ends\".\n",
    "\n",
    "#### Pros\n",
    "\n",
    " - Free\n",
    " - Makes a REST API interface, so you can demo with Postman or an HTML + JS web frontend (or collaborate with an SE alum to make a React frontend for you)\n",
    " - Only requires you to learn a little bit of Flask.  Nothing about production WSGI servers, or nginx, or ports.\n",
    "\n",
    "#### Cons\n",
    "\n",
    " - Not making a full-stack web app means you won't have the experience of making a full-stack web app.\n",
    " - Sometimes there is a lot of \"magic\" happening with the implicit Flask app that makes it hard to debug. Probably will not be compatible with TensorFlow, for example, because you don't have fine-grained control over the threading behavior.\n",
    " - If you want to deploy a web frontend, you'll need to find another tool. (GitHub Pages works fine though!)\n",
    " - The platform is not designed specifically for ML models, so it might be annoyingly slow, or even so slow that it times out.\n",
    " - You can't generate any data visualizations in Python; they will have to be in JavaScript.\n",
    "\n",
    "#### Documentation\n",
    "\n",
    "1. [Writing cloud functions](https://cloud.google.com/functions/docs/writing/http)\n",
    "2. [Deploying cloud functions](https://cloud.google.com/functions/docs/deploying/filesystem)\n",
    "3. [Calling cloud functions](https://cloud.google.com/functions/docs/calling/http)\n",
    "\n",
    "#### Example\n",
    "\n",
    "Erin Hoffman has a working example of a Google Cloud function in the `google_cloud_function` folder. You can call it right now by pasting this into your terminal:\n",
    "```\n",
    "curl -X POST -H \"Content-type: application/json\" -d '{\"Alcohol\": 12.82, \"Malic acid\": 3.37, \"Ash\": 2.3, \"Alcalinity of ash\": 19.5, \"Magnesium\": 88.0, \"Total phenols\": 1.48, \"Flavanoids\": 0.66, \"Nonflavanoid phenols\": 0.4, \"Proanthocyanins\": 0.97, \"Color intensity\": 10.26, \"Hue\": 0.72, \"OD280/OD315 of diluted wines\": 1.75, \"Proline\": 685.0}' https://us-central1-splendid-petal-256700.cloudfunctions.net/wine-predictor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML-Specific Deployment Example: Google Cloud AI Platform\n",
    "\n",
    "Google Cloud AI Platform allows you to upload an exported model to the cloud, then make future requests to the cloud to make new predictions. The primary use case seems to be people who are already very integrated into the Google Cloud ecosystem, since they encourage requesting predictions through their custom SDK in Python rather than through a standard REST API interface.\n",
    "\n",
    "#### Pricing\n",
    "\n",
    "It looks like there is no free tier for this service. [Pricing summary here.](https://cloud.google.com/ml-engine/docs/pricing)\n",
    "\n",
    "#### Pros\n",
    "\n",
    " - After installing their CLI, you only need to upload the model itself, no additional libraries or frameworks.\n",
    " - If you need better performance, it's easy to pay more money to get that.\n",
    " - This is a \"real\" tool that you might encounter on the job, and resembles most closely the type of systems administration you might need to do on the job.\n",
    " - If you need to use TensorFlow, Google made both TensorFlow and this product, so they're likely to be the most compatible.\n",
    "\n",
    "#### Cons\n",
    "\n",
    " - Not free.\n",
    " - No clear, easy way to make an \"open\" REST API that can communicate with Postman or a simple web frontend.\n",
    " - Requires versioning and other configuration work that is not necessary for our projects.\n",
    "\n",
    "#### Documentation\n",
    "\n",
    "1. [Exporting models for prediction](https://cloud.google.com/ml-engine/docs/exporting-for-prediction)\n",
    "2. [Deploying models](https://cloud.google.com/ml-engine/docs/deploying-models)\n",
    "3. [Getting online predictions in Python](https://cloud.google.com/ml-engine/docs/online-predict), [Getting online predictions via REST](https://cloud.google.com/ml-engine/docs/v1/predict-request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Examples\n",
    "\n",
    "**Full-Stack Web Development**\n",
    " - If you really want to dive into full-stack web development, [Django](https://www.djangoproject.com/start/) has more features than Flask.\n",
    " - Besides Heroku and AWS EC2, you can also host websites with [Azure App Service](https://docs.microsoft.com/en-us/learn/modules/host-a-web-app-with-azure-app-service/index), [DigitalOcean](https://www.digitalocean.com/community/tutorials/how-to-deploy-a-flask-application-on-an-ubuntu-vps), [Google Cloud App Engine](https://cloud.google.com/python/getting-started/hello-world), and [AWS Elatic Beanstalk](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/Welcome.html).\n",
    " \n",
    "**Cloud Functions**\n",
    " - [AWS Lambda Functions](https://docs.aws.amazon.com/lambda/latest/dg/python-programming-model.html)\n",
    " - [Azure Functions](https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-first-function-python)\n",
    " \n",
    "**ML-Specific Deployment**\n",
    " - [IBM Watson Studio](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/pm_service_supported_frameworks.html)  (kind of a mixture between a cloud function and ML-specific deployment)\n",
    " - [AWS SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html)\n",
    " - [Azure Machine Learning](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oracle Cloud Instances\n",
    "\n",
    "Oracle Cloud Instances are similar to AWS EC2 instances: basically a complete \"computer\" that runs in the cloud, and that you interact with via SSH in the terminal.\n",
    "\n",
    "Oracle seems to be trying to match the services provided by AWS, Google Cloud, and Azure, and is attracting customers by making the free tier of their \"instance\" service pretty fast/powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "\n",
    "These instructions assume that you already have a working, production-ready Flask application, where all of the relevant files (including the pickled model) have been pushed up into a single public GitHub repository.\n",
    "\n",
    "The repository should have a `requirements.txt` in it, which was generated with `pip freeze > requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Making the Cloud Instance\n",
    "\n",
    "Go to the [Oracle Cloud Infrastructure website](https://www.oracle.com/cloud/), click on \"Try Oracle Cloud Free Tier\", click on \"Start for Free\", and go through the account creation flow. It will require you to enter credit card information. Choose the region closest to you.\n",
    "\n",
    "From your homepage, click on \"Create a VM Instance\".\n",
    "\n",
    "The trickiest part here is getting the SSH key connected.  The combination of public and private keys is used _instead of_ a traditional username/password login. You provide the public key when setting up your instance and provide the private key when logging in.\n",
    "\n",
    "Assuming you set up GitHub correctly back in Week 1, you should not need to generate a new SSH key. It should already be located on your computer at `~/.ssh/id_rsa.pub`. Because the `.ssh` directory starts with a `.`, the Mac OS will not automatically show it to you when you click \"Choose Files\", so you need to type `Command+Shift+G` (`⇧⌘G`) from the Finder window and then type `~/.ssh` into the textbox and select `id_rsa.pub` from the folder once it's opened.\n",
    "\n",
    "Once the status of the cloud instance turns green and says RUNNING, you should be able to log into your cloud instance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Logging in\n",
    "\n",
    "Your cloud instance is basically like any other Unix-like computer, which can be accessed via the command line.\n",
    "\n",
    "To connect, first find the public IP address from the \"Instance Details\" page. It is under the \"Primary VNIC Information\" header.\n",
    "\n",
    "In a Terminal window on your local computer, use the SSH command to connect. The format is:\n",
    "```\n",
    "ssh opc@<ip_address> —i <private_key>\n",
    "```\n",
    "\n",
    "(You may not actually need the last part starting with `-i` depending on your computer's configuration, but I think it's helpful to be explicit)\n",
    "\n",
    "Now you should see a command prompt, something like:\n",
    "```\n",
    "[opc@instance-XXXXXXXX-XXXX ~]$\n",
    "```\n",
    "\n",
    "This is a Bash prompt, which works very similarly to the one on your Mac. Try some basic commands like `ls` or `pwd`!\n",
    "\n",
    "You can disconnect by typing `exit` and hitting Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Installing Python, Pip, and Git\n",
    "\n",
    "Oracle Linux uses the [YUM package manager](http://yum.baseurl.org/), which is roughly similar to Homebrew on a Mac.\n",
    "\n",
    "To install Python 3 and Pip 3, run:\n",
    "```\n",
    "sudo yum install python3\n",
    "```\n",
    "\n",
    "If that worked correctly, you should be able to run `which python3` and `which pip3` and it will print out the paths to the executables.\n",
    "\n",
    "To install Git, run:\n",
    "```\n",
    "sudo yum install git\n",
    "```\n",
    "\n",
    "Now if you run `git status` from the root directory, you should get an error message saying that it is not a git repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Cloning the Flask App and Installing Dependencies\n",
    "\n",
    "As long as the repository is public on GitHub, this should work the same as cloning any other repository. Just a typical:\n",
    "```\n",
    "git clone <repository_link>\n",
    "cd <repository_name>\n",
    "```\n",
    "\n",
    "From the repository, you'll want to install of the Python package dependencies.\n",
    "\n",
    "To install all packages specified in `requirements.txt`, run:\n",
    "```\n",
    "sudo pip3 install -r requirements.txt\n",
    "```\n",
    "\n",
    "It will give you a warning message for the reason mentioned above (it's not a best practice to install Pip packages globally rather than using a `conda` or `virtualenv` environment) then proceed to install all of the necessary packages.\n",
    "\n",
    "To test that your Flask app can run without errors, test out running it:\n",
    "```\n",
    "export FLASK_ENV=production\n",
    "python3 app.py\n",
    "```\n",
    "\n",
    "You won't be able to test it in a browser until the network settings are configured, though!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Configuring Network Settings\n",
    "\n",
    "As far as I can tell, you must use two different interfaces to specify which ports should accept incoming requests. For these examples, we are assuming that the default port for Flask is used (port 5000).\n",
    "\n",
    "First, configure from the \"console\" in the browser where you manage cloud instances. Do this by navigating to \"Networking\" --> \"Virtual Cloud Networks\" in the main menu, selecting your network (should be only one, created by default), selecting \"Security Lists\" in the bottom left menu, selecting \"Default Security List\", then clicking the blue \"Add Ingress Rules\" button.  In that pop-up window, enter `0.0.0.0/0` in the SOURCE CIDR field and 5000 in the DESTINATION PORT RANGE field, then click the blue \"Add Ingress Rules\" button at the bottom.\n",
    "\n",
    "Second, configure the firewall on the server itself to accept requests on that port. Making sure that you are SSH-ed in to your instance (seeing the prompt with `opc@instance`), run the lines:\n",
    "```\n",
    "sudo firewall-cmd --zone=public --permanent --add-port=5000/tcp\n",
    "sudo firewall-cmd --reload\n",
    "```\n",
    "\n",
    "Now, if you run the code to start your Flask app again, you should be able to access it in your local browser!  Just type the public IP address followed by the port, e.g. `http://129.146.133.106:5000/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Using `tmux` to Run Indefinitely\n",
    "\n",
    "With the current setup, the web server only runs as long as the SSH session is live. This is fine even for demo-ing at showcase, but if you actually want the website to work whenever someone wants to view it, the web server needs to be detached from the SSH session.\n",
    "\n",
    "The typical tool used for this is [`tmux`](https://tmuxguide.readthedocs.io/en/latest/tmux/tmux.html).\n",
    "\n",
    "Just once, you need to install `tmux`:\n",
    "```\n",
    "sudo yum install tmux\n",
    "```\n",
    "\n",
    "After that, the process for starting your cloud-hosted web server is:\n",
    "```\n",
    "tmux\n",
    "export FLASK_ENV=production\n",
    "python3 app.py\n",
    "```\n",
    "\n",
    "Then press `control-b`, then release both keys, then press `d`, in order to detach from the `tmux` session.\n",
    "\n",
    "Now your web server will keep running, even if you log out of SSH!\n",
    "\n",
    "If you want to log in and stop the web server, or inspect the logs, first SSH back in, then run `tmux attach`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
