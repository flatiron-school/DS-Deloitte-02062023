{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics Code Appendix\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "One of the most popular approaches to evaluate the predictive power of a classification model is to review the confusion matrix. The simplest confusion matrix is for a two-class classification problem, with negative (class 0) and positive (class 1) classes.\n",
    "\n",
    "SKLearn Confusion Matrix:\n",
    "\n",
    "|   | Negative Prediction  |  Positive Prediction  |\n",
    "|  :---:  |  :---:  |  :---:  |\n",
    "| **Negative Class** | True Negative (TN) | False Positive (FP) |\n",
    "| **Positive Class** | False Negative (FN) | True Positive (TP) |\n",
    "\n",
    "Python Code:\n",
    "\n",
    "``` Python\n",
    "# Import dependency\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "# Create the confusion matrix, assume model is trained and prediction is made\n",
    "confusion_matrix(y, y_hat)\n",
    "\n",
    "# Create visualization for the confusion matrix\n",
    "plot_confusion_matrix(model, X, y)\n",
    "```\n",
    "\n",
    "Four major measures we would like to calculate from the confusion matrix to evaluate the classification model are: accuracy, precision, recall, and F-measure.\n",
    "\n",
    "#### Accuracy:\n",
    "\n",
    "Accuracy refers to the overall accuracy rate of the model, which divides the total number of correct prediction by the total number of prediction.\n",
    "\n",
    "$$Accuracy = \\frac{TP + TN}{TP + FP + FN + TN}$$\n",
    "\n",
    "Accuracy could be the first high-level measure to check the overall predictive power of the model.\n",
    "\n",
    "Python Code:\n",
    "\n",
    "``` Python\n",
    "# Assume model is already trained\n",
    "model.score(X, y)\n",
    "\n",
    "# Alternatively, if the model is trained and prediction is made\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, y_hat)\n",
    "```\n",
    "\n",
    "#### Precision:\n",
    "\n",
    "Precision is a metric that quantifies the number of correct positive predictions made.\n",
    "\n",
    "$$Precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "Python Code:\n",
    "\n",
    "``` Python\n",
    "# Import dependency\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Assume model is trained and prediction is made\n",
    "precision_score(y, y_hat, average='binary')\n",
    "```\n",
    "\n",
    "#### Recall: \n",
    "\n",
    "Recall is a metric that quantifies the number of correct positive predictions made out of all positive predictions that should have been made.\n",
    "\n",
    "$$Recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Python Code:\n",
    "\n",
    "``` python\n",
    "# Import dependency\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Assume model is trained and prediction is made\n",
    "recall_score(y, y_hat, average='binary')\n",
    "```\n",
    "\n",
    "#### F-Measure / F-score\n",
    "\n",
    "F-score provides a way to combine both precision and recall into a single measure that captures both properties.\n",
    "\n",
    "$$F-score = \\frac{2 \\cdot Precision \\cdot Recall}{Precision + Recall}$$\n",
    "\n",
    "This is the harmonic mean of the two fractions, which is the most common metric used on imbalanced classification problems.\n",
    "\n",
    "Python Code:\n",
    "\n",
    "``` Python\n",
    "# Import dependency\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assume model is trained and prediction is made\n",
    "f1_score(y, y_hat, average='binary')\n",
    "```\n",
    "\n",
    "If we want to change the weight of Recall and Precision in the F-Measure, we can use the $F_{\\beta}$ score for adjusting the weight in the calculation. The general idea is that $\\beta$ can be any value from 0 to $+\\infty$. 0 means put all weight on Precision, $+\\infty$ means put all weight on Recall, $\\beta < 1$ means more weight on Precision, and $\\beta > 1$ means more weight on Recall. When $\\beta = 1$, it tunes the measure back to standard F-Measure. \n",
    "\n",
    "$$F_{\\beta}-score = \\frac{(1+\\beta^2) \\cdot Precision \\cdot Recall}{\\beta^2 \\cdot Precision + Recall}$$\n",
    "\n",
    "Python Code:\n",
    "\n",
    "``` Python\n",
    "# Import dependency\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Assume model is trained and prediction is made\n",
    "fbeta_score(y, y_hat, average='binary', beta=2)\n",
    "```\n",
    "\n",
    "#### Which metric should be used?\n",
    "\n",
    "**Accuracy**: widely used to summarize model performance.\n",
    "\n",
    "**Precision**: appropriate when minimizing false positives is the focus\n",
    "\n",
    "**Recall**: appropriate when minimizing false negatives is the focus\n",
    "\n",
    "**F-Measure**: appropriate when both precision and recall are important for a business problem, so we want to achieve the balance of the two at the same time.\n",
    "\n",
    "SKLearn has a nice function, which returns the accuracy, precision, recall, and F-measure in one summary report.\n",
    "\n",
    "Python Code:\n",
    "\n",
    "``` Python\n",
    "# Import dependency\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assume model is trained and prediction is made\n",
    "classification_report(y, y_hat)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-03T14:17:20.896359Z",
     "start_time": "2021-09-03T14:17:20.893648Z"
    }
   },
   "outputs": [],
   "source": [
    "# Actual classes vs Predicted classes\n",
    "y_true = [0, 1, 2, 0, 1, 2]\n",
    "y_pred = [0, 2, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the F-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the F_beta-score with beta = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the calssification report for summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Matrix\n",
    "\n",
    "A cost Matrix is used to choose the model that better handles FP and FN in the model predictions. We can assign a weight of the cost for each type of (false) prediction in the confusion matrix, so that we can pick a model that minimzes our cost.\n",
    "\n",
    "For example, we want to reduce the risk of FP in the model prediction in the following example, so we assign a higher weight to FP in the cost matrix.\n",
    "\n",
    "**Cost Matrix:**\n",
    "\n",
    "|  | Pred 0 | Pred 1 |\n",
    "| :---: | :---: | :---: |\n",
    "| Act 0 | 0 | 10 |\n",
    "| Act 1 | 2 | 0 |\n",
    "\n",
    "Now let's consider two confusion matrices from two different models and the accuracy rate and cost value from each confusion matrix.\n",
    "\n",
    "**Model 1 Confusion Matrix:**\n",
    "\n",
    "|  | Pred 0 | Pred 1 |\n",
    "| :---: | :---: | :---: |\n",
    "| Act 0 | 125 | 5 |\n",
    "| Act 1 | 27 | 235 |\n",
    "\n",
    "$$Accuracy = \\frac{125 + 235}{125 + 5 + 27 + 235} \\approx 0.9184$$\n",
    "\n",
    "$$Cost = (125 x 0) + (5 x 10) + (27 x 2) + (235 x 0) = 104$$\n",
    "\n",
    "**Model 2 Confusion Matrix:**\n",
    "\n",
    "|  | Pred 0 | Pred 1 |\n",
    "| :---: | :---: | :---: |\n",
    "| Act 0 | 135 | 10 |\n",
    "| Act 1 | 7 | 240 |\n",
    "\n",
    "$$Accuracy = \\frac{135 + 240}{125 + 10 + 7 + 235} \\approx 0.9566$$\n",
    "\n",
    "$$Cost = (135 x 0) + (10 x 10) + (7 x 2) + (240 x 0) = 114$$\n",
    "\n",
    "If we are selecting a model for prediction based on the Accuracy Rate, we would pick Model 2. However, if we are considering the cost matrix and putting more weight on the cost with FP, we would have picked Model 1 for the objective.\n",
    "\n",
    "Note: SKLearn does not have any module or package for Cost Matrix, but it's obviously not a difficult task to create the relevant code in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
