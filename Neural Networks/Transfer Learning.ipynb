{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oiw6QFqccsHG"
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "## Objectives\n",
    "\n",
    "*   Further solidify intuition behind CNNs\n",
    "*   Understand the transfer learning workflow\n",
    "*   Provide an opportunity to practice transfer learning\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rTFcVgzcsHH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHQaEysYcsHI"
   },
   "source": [
    "The idea behind transfer learning is simply to make use of some pre-trained model to make predictions, rather than building a new model from scratch.\n",
    "\n",
    "The plain fact is that several very powerful image-processing networks have already been built and perfected by scientists who have detailed knowledge about how all the layers of their models work. Moreover, many successful models have been trained on hundreds of thousands if not millions of images, and so they could be used for your images as well.\n",
    "\n",
    "In general, the target will of course be different from the original that was used in training the model in the first place. But the idea is that the model will be good at picking up on the *deep features* of images, and so we can use *most* of the pre-trained model, in order to extract those deep features, and then just stick on a couple extra layers at the end that are appropriate for the data we have.\n",
    "\n",
    "In what follows here we'll try building a network from scratch on some chest X-ray data. And then we'll see if we can get better accuracy by using [Imagenet](https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/), a leading CNN for image recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6Vay-pwcsHI"
   },
   "source": [
    "## CNN from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asOQ3PrScsHI"
   },
   "source": [
    "Let's look at some X-rays of lungs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2didOzOz3gck",
    "outputId": "8bf97d96-0b1e-459c-d4aa-705f58ad181d"
   },
   "outputs": [],
   "source": [
    "# Pulling the data into our working directory from our repo using wget\n",
    "\n",
    "!wget https://github.com/flatiron-school/DS-Deloitte-07062022/raw/main/supplemental/chest_xray.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-f4YKLnz3n2-",
    "outputId": "525a0883-8b8b-4274-80eb-9fa99f2689d3"
   },
   "outputs": [],
   "source": [
    "# Unzipping the downloaded data\n",
    "\n",
    "!unzip ./chest_xray.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jtly43yX4qGB"
   },
   "outputs": [],
   "source": [
    "# Removing unnecessary folders\n",
    "\n",
    "!rm -r chest_xray.zip\n",
    "!rm -r sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztDNsG_VcsHI"
   },
   "outputs": [],
   "source": [
    "# Instantiating pre-established train, test, val splits as objects\n",
    "\n",
    "train_f = './chest_xray/train/'\n",
    "test_f = './chest_xray/test/'\n",
    "val_f = './chest_xray/val'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrcCjJzrcsHJ"
   },
   "source": [
    "Keras's ImageDataGenerator can convert images (we have JPEGs here) to tensors of visual information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HzxiWIKcsHJ",
    "outputId": "43fe50a3-0cd8-43db-f1c4-e25ebe57fb07"
   },
   "outputs": [],
   "source": [
    "# Generating resized data for downstream augmentation\n",
    "\n",
    "test_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        test_f, \n",
    "        target_size=(64, 64)) # Since the pipeline processes batches of images that must all have the same size, this must be provided.\n",
    "                              # Size to resize images to after they are read from disk, specified as (height, width)\n",
    "\n",
    "val_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        val_f, \n",
    "        target_size=(64, 64)) \n",
    "\n",
    "train_generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        train_f, \n",
    "        target_size=(64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0ONFg0TcsHJ"
   },
   "source": [
    "ImageDataGenerator uses *data augmentation*, which means that it will take each image and transform it in various ways, ultimately using *only these transformations* as training data. [Here's](https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/) a nice resource on keras's `ImageDataGenerator`.\n",
    "\n",
    "And [here](https://bair.berkeley.edu/blog/2019/06/07/data_aug/) is a page with more information about data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsDF_2YJcsHK"
   },
   "outputs": [],
   "source": [
    "# Using an iterator to generate images and their labels\n",
    "# from train, test, and val image subsets\n",
    "\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)\n",
    "val_images, val_labels = next(val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "E73MVNcecsHK",
    "outputId": "3b21886d-c2d5-42a9-cf81-243a1ac7f7b4"
   },
   "outputs": [],
   "source": [
    "# Plotting scaled image output\n",
    "\n",
    "plt.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHsMibYbcsHK"
   },
   "source": [
    "### Model Building\n",
    "\n",
    "![](https://cs231n.github.io/assets/cnn/cnn.jpeg)\n",
    "\n",
    "Source: Stanford's Convolutional Neural Networks for Visual Recognition Course Notes\n",
    "\n",
    "Even though CNNs are uniquely suited to the problem of image classification -- their architectures make the explicit assumption that the inputs are images, which allows for certain properties to be encoded into the architecture and for the forward function to be more efficient to implement, thereby vastly reducing the amount of parameters in the network -- ***it is not considered to be in line with best practices to code a CNN from scratch for the purpose of building a custom image classifier***. Instead, it is important to point out here that the typical workflow would be more centered on using the weights from a prettrained model to fine-tine a classifier on images with classes similar to those seen in the original training set (i.e., [ImageNet's 1000 classes](https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/)), a process known as transfer learning.\n",
    "\n",
    "For more guidance on CNN architecture best practices, [this open-source Stanford course material](https://cs231n.github.io/convolutional-networks/) provides some good additional insights that are beyond the scope of what is required for this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdeEr2q2csHK"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(64, 64, 3)))\n",
    "                                      \n",
    "model.add(MaxPooling2D((2, 2)))              \n",
    "\n",
    "model.add(Conv2D(32, (4, 4), activation='relu')) \n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"sgd\",\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dEFeShu2e1hL",
    "outputId": "b04e7901-fa7e-4a1a-a5c8-e8647cef50a2"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXwoFulpcsHK",
    "outputId": "9193fddf-57ea-40a2-d8a9-13a420f4d64a"
   },
   "outputs": [],
   "source": [
    "history_log = model.fit(train_images,\n",
    "                    train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGJdGchOcsHL"
   },
   "source": [
    "Note the acc and val_acc scores!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOV8NDAdcsHL"
   },
   "source": [
    "## Now with Transfer Learning!\n",
    "\n",
    "### The typical transfer-learning workflow\n",
    "This leads us to how a typical transfer learning workflow can be implemented in Keras:\n",
    "\n",
    "\n",
    "\n",
    "1.   Instantiate a base model and load pre-trained weights into it.\n",
    "2.   Freeze all layers in the base model by setting `trainable = False`.\n",
    "3.   Create a new model on top of the output of one (or several) layers from the base model.\n",
    "4.   Train your new model on your new dataset.\n",
    "\n",
    "Note that an alternative, more lightweight workflow could also be:\n",
    "\n",
    "1.   Instantiate a base model and load pre-trained weights into it.\n",
    "2.   Run your new dataset through it and record the output of one (or several) layers from the base model. This is called *feature extraction*.\n",
    "3.   Use that output as input data for a new, smaller model.\n",
    "\n",
    "A key advantage of that second workflow is that you only run the base model once on your data, rather than once per epoch of training. So it's a lot faster & cheaper.\n",
    "\n",
    "An issue with that second workflow, though, is that it doesn't allow you to dynamically modify the input data of your new model during training, which is required when doing data augmentation, for instance. Transfer learning is typically used for tasks when your new dataset has too little data to train a full-scale model from scratch, and in such scenarios data augmentation is very important. So in what follows, we will focus on the first workflow.\n",
    "\n",
    "[Source: Keras' Transfer Learning Guide](https://keras.io/guides/transfer_learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bZqrB7McsHL"
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cczi1SAbcsHL"
   },
   "source": [
    "This tool comes from the [Visual Geometry Group](http://www.robots.ox.ac.uk/~vgg/research/very_deep/). More info about the pretrained model can be found in [Keras' VGG documentation](https://keras.io/api/applications/vgg/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IAM43hlgcsHL",
    "outputId": "11501861-07a7-4bbb-be6f-0746cd09e508",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_base = VGG19(weights='imagenet',\n",
    "                  include_top=False, # Excludes top (output) layer(s)\n",
    "                  input_shape=(64, 64, 3)\n",
    "                )\n",
    "\n",
    "cnn_base.trainable = False # Freeze the base model\n",
    "\n",
    "cnn_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udj-kiwAHKAm"
   },
   "source": [
    "#### Specifying Batch Size and Performing Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYnjqSpqcsHL"
   },
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZlZly0KcsHL"
   },
   "outputs": [],
   "source": [
    "def extract_features(directory, sample_amount):\n",
    "    features = np.zeros(shape=(sample_amount, 2, 2, 512)) \n",
    "    labels = np.zeros(shape=(sample_amount))\n",
    "    generator = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
    "        directory, target_size=(64, 64), \n",
    "        batch_size=batch_size, \n",
    "        class_mode='binary')\n",
    "    i=0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = cnn_base.predict(inputs_batch)\n",
    "        features[i * batch_size: (i + 1) * batch_size] = features_batch \n",
    "        labels[i * batch_size: (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_amount:\n",
    "            break\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNBByZVXcsHL",
    "outputId": "82610d86-b726-403e-95ff-e84382a70f6f"
   },
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features(train_f, 5216) \n",
    "validation_features, validation_labels = extract_features(val_f, 16) \n",
    "test_features, test_labels = extract_features(test_f, 624)\n",
    "\n",
    "train_features = np.reshape(train_features, (5216, 2048))\n",
    "validation_features = np.reshape(validation_features, (16, 2048))\n",
    "test_features = np.reshape(test_features, (624, 2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a615ZyxicsHL",
    "outputId": "385238df-f8af-4e14-a092-5f8ef96a5546"
   },
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udu1Rif0HnQd"
   },
   "source": [
    "### Let's Build Our Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxp81S68csHM"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_dim=2048))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJ4v-DYwcsHM",
    "outputId": "84c47362-150c-4cdf-d57c-f051d6d0b0b3"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=RMSprop(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=10,\n",
    "                    batch_size=10,\n",
    "                    validation_data=(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6eHbMmmQd_b"
   },
   "source": [
    "### Fine-Tuning\n",
    "\n",
    "Once your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate.\n",
    "\n",
    "This is an optional last step that can potentially give you incremental improvements. It could also potentially lead to quick overfitting -- keep that in mind.\n",
    "\n",
    "It is critical to only do this step after the model with frozen layers has been trained to convergence. If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features.\n",
    "\n",
    "It's also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to readapt the pretrained weights in an incremental way.\n",
    "\n",
    "Source: Stanford's Convolutional Neural Networks for Visual Recognition Course Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8jeQ_nfOhDD",
    "outputId": "ebe074e9-4b89-44ab-9291-c1319757d878"
   },
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "cnn_base.trainable = True\n",
    "\n",
    "# It's important to recompile your model after you make any changes\n",
    "# to the `trainable` attribute of any inner layer, so that your changes\n",
    "# are take into account\n",
    "model.compile(optimizer=RMSprop(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Train end-to-end. Be careful to stop before you overfit!\n",
    "# Callbacks (i.e., early stopping, etc.) might be considered to regularize...\n",
    "model.fit(train_features, train_labels, epochs=10, validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9EY6l68Qmb4"
   },
   "source": [
    "Yay, it doesn't seem that we overfit! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBSURDqYcsHM"
   },
   "source": [
    "## Explore\n",
    "What other networks are available inside keras?!\n",
    "\n",
    "Check out the Keras docs for some [usage examples](https://keras.io/api/applications/#usage-examples-for-image-classification-models)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_HRntAncsHM"
   },
   "outputs": [],
   "source": [
    "from keras.applications import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQbg90TtcsHM"
   },
   "outputs": [],
   "source": [
    "# Exercise: Use transfer learning with another pre-trained CNN on these data.\n",
    "# See if you can improve on our metrics!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
