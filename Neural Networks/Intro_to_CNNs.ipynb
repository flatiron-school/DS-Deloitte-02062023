{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSPCom-KmApV"
   },
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZG0n47ZZ6sq"
   },
   "source": [
    "Much of the code in this notebook comes from [tensorflow's GitHub](https://github.com/tensorflow/docs/tree/master/site/en/tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDkoQbkNZ6sq"
   },
   "source": [
    "## Learning Goals\n",
    "\n",
    "- describe the types of layers that are distinctive for convolutional nets;\n",
    "- utilize `tensorflow` to build CNNs;\n",
    "- evaluate CNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTB08i04Z6sq"
   },
   "source": [
    "## What are CNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4IPbaOvZ6sq"
   },
   "source": [
    "From [Wikipedia](https://en.wikipedia.org/wiki/Convolutional_neural_network):\n",
    "\n",
    "- \"CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"fully-connectedness\" of these networks makes them prone to overfitting data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. However, CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns.\"\n",
    "<br/>\n",
    "<br/>\n",
    "- \"Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDMA3jcUZ6sr"
   },
   "source": [
    "### Convolving and Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JL2gBQjgZ6sr"
   },
   "source": [
    "The two distinctive types of layer inside of a typical CNN (and there may be several of each in a single network) are **convolutional** and **pooling** layers. Let's look at each in turn.\n",
    "\n",
    "\n",
    "#### Convolution\n",
    "Convolutional nets employ [convolutions](https://en.wikipedia.org/wiki/Convolution), which are a certain kind of transformation. In the context of neural networks processing images, this can be thought of as sliding a filter (of weights) over the image matrix to produce a new matrix of values. (We'll detail the calculation below.) The relative smallness of the filter means both that there will be relatively few parameters to learn and that the values representing certain areas of the image will be affected only by the values of *nearby areas*. This helps the network in **feature detection**. Let's check out some visualizations [here](https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/).\n",
    "\n",
    "Utkarsh Sinha shows us some examples of different kinds of filters [here](http://aishack.in/tutorials/convolutions/).\n",
    "\n",
    "Suppose we have a 3x3 image and a 2x2 filter. Then the calculation goes as follows:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "a & b & c \\\\\n",
    "d & e & g \\\\\n",
    "h & j & k\n",
    "\\end{bmatrix} *\n",
    "\\begin{bmatrix}\n",
    "f_1 & f_2 \\\\\n",
    "f_3 & f_4\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "f_1a + f_2b + f_3d + f_4e & f_1b + f_2c + f_3e + f_4g \\\\\n",
    "f_1d + f_2e + f_3h + f_4j & f_1e + f_2g + f_3j + f_4k\n",
    "\\end{bmatrix}$.\n",
    "\n",
    "In words: Line up the filter with the image, multiply all the corresponding pairs and then add up those products. Repeat for all positions of the filter as allowed by [the stride and the padding](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/). The relative position of the filter to the image will tell you which entry in the resultant matrix you're filling in.\n",
    "\n",
    "##### Exercise\n",
    "Let's try an example of horizontal edge detection. One good filter for that might look like:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "10 & 10 & 10 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-10 & -10 & -10\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Suppose we apply this filter to (i.e. *convolve*) an image with a clear horizontal edge, such as this one:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "200 & 200 & 200 & 200 & 200 \\\\\n",
    "200 & 200 & 200 & 200 & 200 \\\\\n",
    "200 & 200 & 200 & 200 & 200 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<details><summary>\n",
    "    Answer here\n",
    "    </summary>\n",
    "    <br/>\n",
    "    $\\begin{bmatrix}\n",
    "    0 & 0 & 0 \\\\\n",
    "    6000 & 6000 & 6000 \\\\\n",
    "    6000 & 6000 & 6000 \\\\\n",
    "    0 & 0 & 0\n",
    "    \\end{bmatrix}$\n",
    "    Notice how the edge is now \"highlighted\"!\n",
    "    </details>\n",
    "\n",
    "[Here](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) is another good resource.\n",
    "\n",
    "#### Pooling\n",
    "What is pooling? The main goal in inserting a pooling layer is to reduce dimensionality, which helps to reduce both network computation and model overfitting. This is generally a matter of reducing a matrix or tensor of values to  some smaller size, and the most common way of doing this is by partitioning the large matrix into $n$ x $n$ blocks and then replacing each with the largest value in the block. Hence we speak of \"MaxPooling\".\n",
    "\n",
    "Let's check out [this page](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/pooling_layer.html) for some visuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6Cpa3vsZ6ss"
   },
   "source": [
    "## From the TensorFlow Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLGkt5qiyz4E"
   },
   "source": [
    "This tutorial demonstrates training a simple [Convolutional Neural Network](https://developers.google.com/machine-learning/glossary/#convolutional_neural_network) (CNN) to classify MNIST digits. This simple network will achieve over 99% accuracy on the MNIST test set. Because this tutorial uses the [Keras Sequential API](https://www.tensorflow.org/guide/keras), creating and training our model will take just a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7KBpffWzlxH"
   },
   "source": [
    "### Import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAve6DCL4JH4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRFxccghyMVo"
   },
   "source": [
    "### Download and prepare the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWoEqyMuXFF4"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ho-4imMgZ6st"
   },
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PM8UtbtsZ6st"
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PhzNb-WpZ6st"
   },
   "outputs": [],
   "source": [
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255, test_images / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oewp-wYg31t9"
   },
   "source": [
    "### Create the convolutional base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hQvqXpNyN3x"
   },
   "source": [
    "The 6 lines of code below define the convolutional base using a common pattern: a stack of [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [MaxPooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers.\n",
    "\n",
    "As input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size. If you are new to color channels, MNIST has one (because the images are grayscale), whereas a color image has three (R,G,B). In this example, we will configure our CNN to process inputs of shape (28, 28, 1), which is the format of MNIST images. We do this by passing the argument `input_shape` to our first layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uzo4ZcHrMGwC"
   },
   "source": [
    "Apply filters/feature detectors to the input image $\\rightarrow$ generate the feature maps or the activation maps using the Relu activation function.\n",
    "\n",
    "Feature detectors/filters help identify image features (i.e., edges, vertical lines, horizontal lines, bends, etc.).\n",
    "\n",
    "Pooling is then applied over the feature maps for invariance to translation (meaning that if we translate the inputs the CNN will still be able to detect the class to which the input belongs).\n",
    "\n",
    "Pooling is based on the concept that when we change the input by a small amount, the pooled outputs do not change. We can use min pooling, average pooling, or max pooling; max pooling is said to provide better performance though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9YmGQBQPrdn"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(filters=32,\n",
    "                        kernel_size=(3, 3),\n",
    "                        activation='relu',\n",
    "                        input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvDVFkg-2DPm"
   },
   "source": [
    "Let display the architecture of our model so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-C4XBg4UTJy",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEofpfwUZ6su"
   },
   "source": [
    "The number of parameters depends on the number of input and output channels of the layer in question. For more, see [this post](https://medium.com/@zhang_yang/number-of-parameters-in-dense-and-convolutional-neural-networks-34b54c2ec349) and [this post](https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_j-AXYeZ2GO5"
   },
   "source": [
    "Above, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as we go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically,  as the width and height shrink, we can afford (computationally) to add more output channels in each Conv2D layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_v8sVOtG37bT"
   },
   "source": [
    "### Add Dense layers on top\n",
    "To complete our model, we will feed the last output tensor from the convolutional base (of shape (3, 3, 64)) into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, we will flatten (or unroll) the 3D output to 1D,  then add one or more Dense layers on top. MNIST has 10 output classes, so we use a final Dense layer with 10 outputs and a softmax activation.\n",
    "\n",
    "We finally flatten the input and pass these flattened inputs to the network to output the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRs95d6LUVEi"
   },
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipGiQMcR4Gtq"
   },
   "source": [
    " Here's the complete architecture of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Yu_m-TZUWGX"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNKXi-Gy3RO-"
   },
   "source": [
    "As you can see, our (3, 3, 64) outputs were flattened into vectors of shape (576) before going through two Dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3odqfHP4M67"
   },
   "source": [
    "### Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdDzI75PUXrG"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cf_ii4k5Z6sv"
   },
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs=10, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKgyC5K_4O0d"
   },
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtyDF0MKUcM7"
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LvwaKhtUdOo"
   },
   "outputs": [],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cfJ8AR03gT5"
   },
   "source": [
    "As you can see, our simple CNN has achieved a really high test accuracy. Not bad for a few lines of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmqUaCi4Z6sv"
   },
   "source": [
    "## Checking a Particular Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdlcTVraZ6sv"
   },
   "outputs": [],
   "source": [
    "model.predict(test_images[0].reshape(1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5AbeyfTZ6sv"
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_images[0].reshape(28, 28));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viOr_TNAZ6sw"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oICKwaIRaSe7"
   },
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORuneuqSabmk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3YcqLw_L8z0"
   },
   "source": [
    "**Neural networks are effectively black box algos, and learned features in a neural network are therefore not interpretable. An input is passed and the model returns the results.**\n",
    "\n",
    "But we'll try, borrowing from some of the workflow described [here](https://github.com/arshren/Feature-Visualization/blob/master/Feature%20Visualization.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTsf6EJzEpkm"
   },
   "outputs": [],
   "source": [
    "def plotFilters(conv_filter):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(5,5))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( conv_filter, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RO_bOtLYEs3A"
   },
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    if 'conv' in layer.name:\n",
    "        filters, bias= layer.get_weights()\n",
    "        print(layer.name, filters.shape)\n",
    "         #normalize filter values between  0 and 1 for visualization\n",
    "        f_min, f_max = filters.min(), filters.max()\n",
    "        filters = (filters - f_min) / (f_max - f_min)  \n",
    "        print(filters.shape[3])\n",
    "        axis_x=1\n",
    "        #plotting all the filters\n",
    "        for i in range(filters.shape[3]):\n",
    "        #for i in range(6):\n",
    "            #get the filters\n",
    "            filt=filters[:,:,:, i]\n",
    "            plotFilters(filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOvrAHSCE-al"
   },
   "outputs": [],
   "source": [
    "#Visualizing the filters\n",
    "#plt.figure(figsize=(5,5))\n",
    "for layer in model.layers:\n",
    "    if 'conv' in layer.name:\n",
    "        weights, bias= layer.get_weights()\n",
    "        print(layer.name, weights.shape)\n",
    "         #normalize filter values between  0 and 1 for visualization\n",
    "        f_min, f_max = weights.min(), weights.max()\n",
    "        filters = (weights - f_min) / (f_max - f_min)  \n",
    "        print(weights.shape[3])\n",
    "        filter_cnt=1\n",
    "        #plotting all the filters\n",
    "        for i in range(filters.shape[3]):\n",
    "        #for i in range(6):\n",
    "            #get the filters\n",
    "            filt=filters[:,:,:, i]\n",
    "            #plotting ecah channel\n",
    "            for j in range(filters.shape[0]):\n",
    "                #plt.figure( figsize=(5, 5) )\n",
    "                #f = plt.figure(figsize=(10,10))\n",
    "                ax= plt.subplot(filters.shape[3], filters.shape[0], filter_cnt  )\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                plt.imshow(filt[:, j])\n",
    "                filter_cnt+=1\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNNy9m2rGOcl"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
    "\n",
    "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
    "#visualization_model = Model(img_input, successive_outputs)\n",
    "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
    "#Load the input image\n",
    "img = test_images[0]\n",
    "# Convert ht image to Array of dimension (150,150,3)\n",
    "x   = img_to_array(img)                           \n",
    "x   = x.reshape((1,) + x.shape)                   \n",
    "# Rescale by 1/255\n",
    "x /= 255.0\n",
    "# Let's run input image through our vislauization network\n",
    "# to obtain all intermediate representations for the image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "# Retrieve are the names of the layers, so can have them as part of our plot\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    "  print(feature_map.shape)\n",
    "  if len(feature_map.shape) == 4:\n",
    "    \n",
    "    # Plot Feature maps for the conv / maxpool layers, not the fully-connected layers\n",
    "   \n",
    "    n_features = feature_map.shape[-1]  # number of features in the feature map\n",
    "    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n",
    "    \n",
    "    # We will tile our images in this matrix\n",
    "    display_grid = np.zeros((size, size * n_features))\n",
    "    \n",
    "    # Postprocess the feature to be visually palatable\n",
    "    for i in range(n_features):\n",
    "      x  = feature_map[0, :, :, i]\n",
    "      x -= x.mean()\n",
    "      x /= x.std ()\n",
    "      x *=  64\n",
    "      x += 128\n",
    "      x  = np.clip(x, 0, 255).astype('uint8')\n",
    "      # Tile each filter into a horizontal grid\n",
    "      display_grid[:, i * size : (i + 1) * size] = x \n",
    "\n",
    "    # Display the grid\n",
    "    scale = 20. / n_features\n",
    "    plt.figure( figsize=(scale * n_features, scale) )\n",
    "    plt.title ( layer_name )\n",
    "    plt.grid  ( False )\n",
    "    plt.imshow( display_grid, aspect='auto', cmap='viridis' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEWksa88diYY"
   },
   "source": [
    "# LEVEL UP: Visualizing Model Architecture\n",
    "\n",
    "We learned previously how we can use [TensorBoard](https://tensorboard.dev/experiment/HLI3H1nYTAaOYRSB36FgEg/#scalars) to help us better understand our model. Below are a few visualization techniques that enhance our ability to communicate technical details such as model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOqKDtw7dwtK"
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model) # also outputs model.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJtqU33Isw39"
   },
   "source": [
    "## `visualkeras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUnmGg4EeGez"
   },
   "outputs": [],
   "source": [
    "# !pip install visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpfPybKRe6Y7"
   },
   "outputs": [],
   "source": [
    "import visualkeras\n",
    "visualkeras.layered_view(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OA5TZuzIn9o-"
   },
   "outputs": [],
   "source": [
    "visualkeras.layered_view(model, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ivHBjRpodiR"
   },
   "outputs": [],
   "source": [
    "visualkeras.layered_view(model, legend=True, draw_volume=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHZs6_BQs6lZ"
   },
   "source": [
    "## `ann_visualizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0AUEq2rfd2d"
   },
   "outputs": [],
   "source": [
    "# !pip install ann_visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swckEldve_Yh"
   },
   "outputs": [],
   "source": [
    "from ann_visualizer.visualize import ann_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbfDzasFfkrB"
   },
   "outputs": [],
   "source": [
    "ann_viz(model, view = True, filename = 'Model Viz', title='CNN Architecture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ff9xS7QHqgdB"
   },
   "outputs": [],
   "source": [
    "from graphviz import Source\n",
    "path = 'Model Viz'\n",
    "s = Source.from_file(path, format = 'png')\n",
    "s.view()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
