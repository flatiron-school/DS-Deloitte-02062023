{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e8070c2",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "* [Elastice MapReduce (EMR) Overview](#EMR-Overview)\n",
    "* [Getting Started with AWS EMR](#Getting-Started)\n",
    "    * [Create Key Pair for SSH](#Set-Up-an-EC2-Key-Pair-for-SSH)\n",
    "    * [Create S3 Bucket](#Create-an-S3-Bucket)\n",
    "    * [Set up EMR Cluster](#Set-up-and-Run-an-EMR-Cluster)\n",
    "    * [Terminate Cluster](#Terminate-Cluster)\n",
    "* [Inspect Results](#Inspect-Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e3bb64",
   "metadata": {},
   "source": [
    "# EMR Overview\n",
    "\n",
    "<table width = \"70%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"images/hadoop-logo.png\"/> \n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/spark-logo.png\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "**Hadoop:** Hadoop got its start as a Yahoo project in 2006, which became a top-level Apache open-source project afterwards. It’s a general-purpose form of distributed processing that has several components: the Hadoop Distributed File System (HDFS), stores files in a Hadoop-native format and parallelizes them across a cluster; YARN, a schedule that coordinates application runtimes; and MapReduce, the algorithm that actually processes the data in parallel. Hadoop is built in Java, and accessible through many programming languages, for writing MapReduce code, including Python, through a Thrift client. \n",
    "It’s available either open-source through the Apache distribution, or through vendors such as Cloudera (the largest Hadoop vendor by size and scope), MapR, or HortonWorks. \n",
    "\n",
    "**Spark:** Spark is a newer project, initially developed in 2012, at the AMPLab at UC Berkeley. It’s a top-level Apache project focused on processing data in parallel across a cluster, but the biggest difference is that it works in memory. \n",
    "\n",
    "Whereas Hadoop reads and writes files to HDFS, Spark processes data in RAM using a concept known as an RDD, Resilient Distributed Dataset. Spark can run either in stand-alone mode, with a Hadoop cluster serving as the data source, or in conjunction with Mesos. In the latter scenario, the Mesos master replaces the Spark master or YARN for scheduling purposes. \n",
    "Spark is structured around Spark Core, the engine that drives the scheduling, optimizations, and RDD abstraction, as well as connects Spark to the correct filesystem (HDFS, S3, RDBMS, or Elasticsearch). There are several libraries that operate on top of Spark Core, including Spark SQL, which allows you to run SQL-like commands on distributed data sets, MLLib for machine learning, GraphX for graph problems, and streaming which allows for the input of continually streaming log data. \n",
    "\n",
    "Below is a table of differences between Spark and Hadoop: \n",
    "\n",
    "<table width = \"70%\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Hadoop</th>\n",
    "      <th>Spark</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Hadoop is an open source framework which uses a MapReduce algorithm.</td>\n",
    "      <td>Spark is lightning fast cluster computing technology, which extends the MapReduce model to efficiently use with more type of computations.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Hadoop’s MapReduce model reads and writes from a disk, thus slow down the processing speed.</td>\n",
    "      <td>Spark reduces the number of read/write cycles to disk and store intermediate data in-memory, hence faster-processing speed.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Hadoop is designed to handle batch processing efficiently.</td>\n",
    "      <td>Spark is designed to handle real-time data efficiently.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Hadoop is a high latency computing framework, which does not have an interactive mode.</td>\n",
    "      <td>Spark is a low latency computing and can process data interactively.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>With Hadoop MapReduce, a developer can only process data in batch mode only.</td>\n",
    "      <td>Spark can process real-time data, from real time events like twitter, facebook.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Hadoop is a cheaper option available while comparing it in terms of cost.</td>\n",
    "      <td>Spark requires a lot of RAM to run in-memory, thus increasing the cluster and hence cost.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>The PageRank algorithm is used in Hadoop.</td>\n",
    "      <td>Graph computation library called GraphX is used by Spark.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "##### [Source](https://www.geeksforgeeks.org/difference-between-hadoop-and-spark/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1761b0",
   "metadata": {},
   "source": [
    "## AWS EMR Landing Page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029fa52a",
   "metadata": {},
   "source": [
    "<img src = \"images/emr.png\" width = \"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f6967",
   "metadata": {},
   "source": [
    "## AWS EMR FAQs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2bc9c",
   "metadata": {},
   "source": [
    "<img src = \"images/emr-overview.png\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa78f03",
   "metadata": {},
   "source": [
    "## AWS EMR Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcbe1ce",
   "metadata": {},
   "source": [
    "<img src = \"images/emr-use-cases.png\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bbad7c",
   "metadata": {},
   "source": [
    "## AWS EMR Features and Benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29809119",
   "metadata": {},
   "source": [
    "<img src = \"images/emr-features-and-benefits.png\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fbf534",
   "metadata": {},
   "source": [
    "For more info, check out the [AWS' EMR page](https://aws.amazon.com/emr/?nc=sn&loc=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2807ee",
   "metadata": {},
   "source": [
    "[Back to TOC](#Contents)\n",
    "\n",
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b14071",
   "metadata": {},
   "source": [
    "## Set Up an EC2 Key Pair for SSH\n",
    "\n",
    "To get setup to run an EMR cluster in the cloud, you first have to [set up an EC2 key pair for SSH](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-setting-up.html#emr-setting-up-key-pair). In other words, in order to authenticate and connect to the nodes in a cluster over a secure channel using the Secure Shell (SSH) protocol, you must create an Amazon Elastic Compute Cloud ([Amazon EC2](https://aws.amazon.com/ec2/)) key pair before you launch the cluster. You can also create a cluster without a key pair. This is usually done with transient clusters that start, run steps, and then terminate automatically.\n",
    "\n",
    "<img src = \"images/ec2-key-pairs.png\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22cb07a",
   "metadata": {},
   "source": [
    "Create a pair called `ems` to be called when setting up and running your EMR cluster:\n",
    "\n",
    "<img src = \"images/create-ec2-pair.png\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e3e7c9",
   "metadata": {},
   "source": [
    "The name `ems` is entered and all default values are kept before clicking the `Create key pair` button in the example below:\n",
    "\n",
    "<img src = \"images/create-ec2-pair-2.png\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d928d",
   "metadata": {},
   "source": [
    "Success!\n",
    "\n",
    "<img src = \"images/created-ec2-pair.png\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ac333",
   "metadata": {},
   "source": [
    "## Create an S3 Bucket\n",
    "\n",
    "Now we must create an S3 bucket, into which you'll upload the data source and [pySpark](https://spark.apache.org/docs/latest/api/python/getting_started/index.html) script:\n",
    "\n",
    "<img src = \"images/s3-buckets.png\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb07a625",
   "metadata": {},
   "source": [
    "Using the `Upload` button in the interface, the `csv` and `.py` files are added to the bucket called `health-violations-script`, whereas the `logs` and `restaurant_violation_results` folders are automatically added as an artifact of the EMR cluster running our script in the cloud:\n",
    "\n",
    "<img src = \"images/s3-bucket.png\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879ccaf",
   "metadata": {},
   "source": [
    "## Set up and Run an EMR Cluster\n",
    "\n",
    "So let's set up our cluster and run it!\n",
    "\n",
    "You'll make a few changes from the default parameters:\n",
    "\n",
    "1. Change the `S3 folder` path to include the name of the bucket you created with `/logs` appended to it, and\n",
    "2. Select `Spark` from the list of `Applications` (see image below):\n",
    "\n",
    "<img src = \"images/create-cluster-1.png\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b04156",
   "metadata": {},
   "source": [
    "3. Select `ems` (or whatever you called the key pair you created at the beginning of the tutorial) from the `EC2 key pair` dropdown, as shown below:\n",
    "\n",
    "<img src = \"images/create-cluster-2.png\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f6418",
   "metadata": {},
   "source": [
    "If successful, you should see this:\n",
    "\n",
    "<img src = \"images/pending.png\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c189a8a6",
   "metadata": {},
   "source": [
    "Finally, you'll add a step by clicking the `Steps` tab at the top of your cluster interface in the console, and then clicking the blue `Add step` button. You'll then make a couple changes to the default parameters, as shown below:\n",
    "\n",
    "<img src = \"images/add-step.png\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745fc1ab",
   "metadata": {},
   "source": [
    "## Terminate Cluster\n",
    "\n",
    "Be sure to terminate your cluster by selecting your cluster in the console interface and clicking the `Terminate` button!\n",
    "\n",
    "<img src = \"images/terminate-cluster.png\" width = \"70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142bd0f1",
   "metadata": {},
   "source": [
    "## Inspect Results\n",
    "\n",
    "Assuming that your cluster ran successfully, you'll be able to access a `csv` file beginning with `part-` from the `restaurant_violations_results` folder in the S3 bucket you created for this tutorial:\n",
    "\n",
    "<img src = \"images/results.png\" width = \"30%\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
